from langchain_community.llms import Ollama
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate

# Conectando ao modelo local
llm = Ollama(model="gemma3:12b")

# Mem贸ria
memory = ConversationBufferMemory()

# Prompt customizado
prompt = PromptTemplate(
    input_variables=["history", "input"],
    template="""
Voc锚 茅 um assistente de intelig锚ncia artificial prestativo, direto e fala em portugu锚s.
Use o hist贸rico da conversa para manter o contexto.

Hist贸rico da conversa:
{history}

Usu谩rio: {input}
IA:"""
)

# Loop de intera莽茫o com streaming
print(" Ol谩! Sou um assistente com resposta em tempo real. Digite 'sair' para encerrar.\n")

while True:
    user_input = input("Voc锚: ")
    if user_input.strip().lower() == "sair":
        print("IA: At茅 mais! ")
        break

    # Gera o prompt completo com hist贸rico
    prompt_text = prompt.format(
        history=memory.buffer,
        input=user_input
    )

    print("IA: ", end="", flush=True)

    # Streaming da resposta token por token
    response = ""
    for chunk in llm.stream(prompt_text):
        print(chunk, end="", flush=True)
        response += chunk

    print()  # Nova linha ap贸s resposta completa

    # Atualiza a mem贸ria com o turno atual
    memory.save_context({"input": user_input}, {"output": response})
